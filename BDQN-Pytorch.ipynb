{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from torch.distributions import normal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from helpers import ReplayBuffer, make_atari, make_gym_env, wrap_deepmind, wrap_pytorch\n",
    "from models import Deep_feature,CnnDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Using GPU: GPU not requested or not available.\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using GPU: GPU requested and available.\")\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    dtypelong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print(\"NOT Using GPU: GPU not requested or not available.\")\n",
    "    dtype = torch.FloatTensor\n",
    "    dtypelong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dimension = 512\n",
    "learning_rate = 0.0025 \n",
    "replay_buffer_size = 1000000\n",
    "max_time_step = 5 * 10**6\n",
    "\n",
    "sigma = 0.001\n",
    "sigma_n = 1\n",
    "\n",
    "start_train_ts = 10*5\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "target_network_update_f = 10000\n",
    "\n",
    "target_W_update = 10 \n",
    "target_batch_size = 5000 \n",
    "\n",
    "from models import Deep_feature\n",
    "env_name = \"PongNoFrameskip-v4\"  # Set the desired environment\n",
    "env = make_atari(env_name)\n",
    "env = wrap_pytorch(wrap_deepmind(env),scale=True)\n",
    "num_action = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(agent, batch_size, replay_buffer, optimizer, gamma):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    state = torch.tensor(np.float32(state)).type(dtype)\n",
    "    next_state = torch.tensor(np.float32(next_state)).type(dtype)\n",
    "    action = torch.tensor(action).type(dtypelong)\n",
    "    reward = torch.tensor(reward).type(dtype)\n",
    "    done = torch.tensor(done).type(dtype)\n",
    "\n",
    "    _, argmax_Q = torch.max(torch.mm(deep_feature(next_state), W_mean.transpose(0, 1)),dim=1,keepdim=True)\n",
    "    Q_target = torch.mm(deep_target_feature(next_state), W_target.transpose(0, 1))\n",
    "    Q_target = torch.gather(Q_target, 1, argmax_Q).squeeze() * (1 - done)\n",
    "    Q = torch.mm(deep_feature(state), W_mean.transpose(0, 1))\n",
    "    Q = torch.gather(Q, 1, action.type(dtypelong).unsqueeze(1)).squeeze()\n",
    "    target = (reward + gamma * Q_target).data\n",
    "    loss = (Q - target).pow(2).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BayesReg(phiphiT, phiY, target_batch_size):\n",
    "    with torch.no_grad():\n",
    "        chunk_size = 1000\n",
    "        num_chunks = int(target_batch_size / chunk_size)\n",
    "\n",
    "_, argmax_Q = torch.max(torch.mm(deep_feature(next_state), W_mean.transpose(0, 1)),dim=1,keepdim=True)\n",
    "Q_target = torch.mm(deep_target_feature(next_state), W_target.transpose(0, 1))\n",
    "Q_target = torch.gather(Q_target, 1, argmax_Q).squeeze() * (1 - done)\n",
    "Q = torch.mm(deep_feature(state), W_mean.transpose(0, 1))\n",
    "Q = torch.gather(Q, 1, action.type(dtypelong).unsqueeze(1)).squeeze()\n",
    "target = (reward + gamma * Q_target).data\n",
    "\n",
    "        for _ in range(num_chunks):\n",
    "            state, action, reward, next_state, done = replay_memory.sample(chunk_size)\n",
    "            _, argmax_Q = torch.max(torch.mm(deep_feature(next_state), W_mean.transpose(0, 1)),dim=1,keepdim=True)\n",
    "            Q_target = torch.mm(deep_target_feature(next_state), W_target.transpose(0, 1))\n",
    "            Q_target = torch.gather(Q_target, 1, argmax_Q).squeeze() * (1 - done)\n",
    "            target = (reward + gamma * Q_target).data\n",
    "            \n",
    "            feature_rep = deep_feature(state).unsqueeze(1).detach()\n",
    "            for i in range(num_action):\n",
    "                action_ = action == i # I am not sure it is a right way of doing it\n",
    "                feature_rep_of_action = torch.mm(feature_rep,action_)\n",
    "                phiphiT[i] = torch.mm(feature_rep_of_action.transpose(0, 1),feature_rep_of_action)\n",
    "                phiY[i] = torch.mm(feature_rep_of_action,target)\n",
    "\n",
    "        for i in range(num_action):\n",
    "            inv = np.linalg.inv(\n",
    "                ((phiphiT[i] / opt.sigma_n + 1 / opt.sigma * eye).cpu()).numpy()\n",
    "            )\n",
    "            E_W[i] = torch.tensor(np.dot(inv, phiY[0].cpu().data) / opt.sigma_n).type(\n",
    "                dtype\n",
    "            )\n",
    "            Cov_W[i] = torch.tensor(opt.sigma * inv).type(dtype)\n",
    "        return phiphiT, phiY, W_mean, Cov_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sample_W(W_mean, Cov_W_decom):\n",
    "    dist = normal.Normal(loc=0, scale=1)\n",
    "    for i in range(num_action):\n",
    "        sam = dist.sample((feature_dimension, 1)).type(dtype)\n",
    "        W[i] = W_mean[i] + torch.mm(Cov_W_decom[i], sam)[:, 0]\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_feature = Deep_feature(env.observation_space.shape,feature_dimension, env.action_space.n)\n",
    "deep_target_feature = deepcopy(deep_feature)\n",
    "\n",
    "optimizer = optim.rmsprop(deep_feature.parameters(), lr=learning_rate)\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye = torch.eye(feature_dimension).type(dtype)\n",
    "dist = normal.Normal(loc=0, scale=0.01)\n",
    "W = dist.sample((num_action, feature_dimension)).type(dtype)\n",
    "W_target = dist.sample((num_action, feature_dimension)).type(dtype)\n",
    "W_mean = dist.sample((num_action, feature_dimension)).type(dtype)\n",
    "Cov_W = torch.eye(feature_dimension).repeat(num_action, 1, 1).type(dtype)\n",
    "Cov_W_decom = Cov_W\n",
    "Cov_W_target = Cov_W\n",
    "phiphiT = torch.zeros((num_action, feature_dimension, feature_dimension)).type(dtype)\n",
    "phiY = torch.zeros((num_action, feature_dimension)).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, all_rewards = [], []\n",
    "state = env.reset()\n",
    "c_t = 0\n",
    "for ts in range(1, max_time_step + 1):\n",
    "    action = torch.mm(W, deep_feature(state).transpose(0, 1)).squeeze()\n",
    "    action = torch.argmax(a)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(int(action.cpu()))\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        W = Sample_W(W_mean, Cov_W_decom)\n",
    "\n",
    "    if ( len(replay_buffer) > start_train_ts) and (ts % target_network_update_f):\n",
    "        loss = compute_td_loss(\n",
    "            agent, batch_size, replay_buffer, optimizer, gamma\n",
    "        )\n",
    "        losses.append(loss.data)\n",
    "    \n",
    "    if ts % target_network_update_f:\n",
    "        for t_param, param in zip(deep_target_feature.parameters(), deep_feature.parameters()):\n",
    "            new_param = param.data\n",
    "            t_param.data.copy_(new_param)\n",
    "        c_t += 1\n",
    "        if c_t == target_W_update:\n",
    "            c_t = 0 \n",
    "            phiphiT, phiY, W_mean, Cov_W = BayesReg(phiphiT, phiY, target_batch_size)\n",
    "            W_target = W_mean\n",
    "            Cov_W_target = Cov_W\n",
    "\n",
    "            for ii in range(num_action):\n",
    "                Cov_W_decom[ii] = torch.tensor(\n",
    "                    np.linalg.cholesky(\n",
    "                        (((Cov_W[ii] + Cov_W[ii].transpose(0, 1))) / 2.0).cpu()\n",
    "                    )\n",
    "                ).type(dtype) # in pytorch has stable cholesky decomposinong, it is better to use it, mxnet did not have\n",
    "        \n",
    "        if len(replay_memory) < 100000:\n",
    "            target_batch_size = len(replay_memory)\n",
    "        else:\n",
    "            target_batch_size = 100000\n",
    "            \n",
    "\n",
    "    if ts % params.log_every == 0:\n",
    "        out_str = \"Timestep {}\".format(ts)\n",
    "        if len(all_rewards) > 0:\n",
    "            out_str += \", Reward: {}\".format(all_rewards[-1])\n",
    "        if len(losses) > 0:\n",
    "            out_str += \", TD Loss: {}\".format(losses[-1])\n",
    "        print(out_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
